We are going to use the Metropolis-Hastings algorithm to generate a Markov Chain with limiting distribution given by the binomial distribution Binomial$(n,p)$ with probability distribution

\begin{equation}
    \pi(x) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x},
\end{equation}

for $x = 0,1, \hdots ,n$, $n\in{\N}$ and $p\in{[0,1]}$. Assuming the present state of the Markov chain is $X_{n-1}$ we propose a new state $X^\star$ based on the following proposal distribution:

\begin{equation}
\begin{split}
    Q(X^\star \given X_{n-1} = 0) &= 
    \begin{cases} 
        \frac{1}{2} & \text{for } X^\star \in{\{0, 1\}} \\
        0 & \text{otherwise }  \\
    \end{cases}
    \\
    Q(X^\star \given X_{n-1} = k) &= 
    \begin{cases} 
        \frac{1}{2} & \text{for } X^\star \in{\{k-1, k+1\}} \\
        0 & \text{otherwise }  \\
    \end{cases}
    \text{  with } 0 < k < n
    \\
    Q(X^\star \given X_{n-1} = n) &= 
    \begin{cases} 
        \frac{1}{2} & \text{for } X^\star \in{\{n-1, n\}} \\
        0 & \text{otherwise.}  \\
    \end{cases}
\end{split}
\end{equation}}

This can also be denoted as a transition probability matrix

\begin{equation}
Q = 
\begin{bmatrix}
    \frac{1}{2} & \frac{1}{2} & 0 & 0 & \hdots & 0 \\
    \frac{1}{2} & 0 & \frac{1}{2} & 0 & \hdots & 0 \\
    0 & \frac{1}{2} & 0 & \frac{1}{2} & \hdots & 0 \\
    \vdots & & \ddots & \ddots & \ddots & \vdots \\
    0 & \hdots & 0 & \frac{1}{2} & 0 & \frac{1}{2}\\
    0 & \hdots & 0 & 0 & \frac{1}{2} & \frac{1}{2} \\
    
\end{bmatrix}
,
\end{equation}

which is clearly irreducible. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
Let $q_{ij}$ denote the row i column j element of $Q$. According to the Metropolis-Hastings algorithm, we can define a time reversible Markov chain ${X_n, n \geq 0}$ as follows: When $X_n = i$, let $Y$ be a random variable such that $Y = j$ with probability $P(Y = y) = q_{ij}$ for $j = 0, 1, \hdots, n$. Then let $P(X_{n+1} = j) = \alpha_{ij}$ and $P(X_{n+1} = i) = 1-\alpha_{ij}$, where we define $\alpha_{ij}$ as follows:

\begin{equation}
\begin{split}
    \alpha_{ij} &= 
    \begin{cases}
        \min \biggl(\dfrac{\pi(j) q_{ji}}{\pi(i) q_{ij}}, 1\biggr) & \text{for } q_{ij} \neq 0 \\
        0 & \text{for } q_{ij} = 0 \\
    \end{cases}
    \\
                &= 
    \begin{cases}
        \min \biggl(\dfrac{i!(n-i)!}{j!(n-j)!} \biggl(\dfrac{1-p}{p}\biggr)^{i-j}, 1\biggr) & \text{for } q_{ij} \neq 0 \\
        0 & \text{for } q_{ij} = 0 \\
    \end{cases}
    ,
\end{split}
\end{equation}

where we have used $q_{ij} = q_{ji}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}

The transition probabilities $P_{ij} = P(X_{n} = j \given X_{n-1} = i)$ of the generated Markov chain are given by

\begin{itemize}
    \item 
    if $i \neq j$:
    \begin{equation}
    \begin{split}
        P_{ij}  &=
            q_{ij} \alpha_{ij}
        \\
                &=
            q_{ij} \min \biggl(\dfrac{i!(n-i)!}{j!(n-j)!} \biggl(\dfrac{1-p}{p}\biggr)^{i-j}, 1\biggr)
        \\
    \end{split}
    \end{equation}
    \item
    if i = j:
    \begin{equation}
    \begin{split}
        P_{ii}  &=
            q_{ii} + \sum_{k \neq i} q_{ik}(1-\alpha_{ik})
        \\
                &=
        \begin{cases}
            1 - \frac{1}{2} \min\biggl(n\dfrac{p}{1-p}, 1\biggr) & \text{for i = 0} \\
            1 - \frac{1}{2} \min\biggl(\dfrac{i}{n-i+1} \dfrac{1-p}{p}, 1\biggr) - \frac{1}{2} \min\biggl(\dfrac{n-i}{i+1} \dfrac{p}{1-p}, 1\biggr) & \text{for } 0<i<n \\
            1 - \frac{1}{2} \min\biggl(n\dfrac{1-p}{p}, 1\biggr) & \text{for i = n} \\
        \end{cases}
    \end{split}
    \end{equation}
\end{itemize}

If $\pi(i)$ for $i \in{0, 1, \hdtos, n}$ are indeed the stationary probabilites of the Markov chain, they should satisfy the following equation:

\begin{equation}
    \label{eq:statprob}
    \pi(i) P_{ij} = \pi(j) P_{ji} \iff \pi(i) q_{ij} \alpha_{ij} = \pi(j) q_{ji} \alpha_{ji}
\end{equation}

For all $i = j$ and for all $q_{ij} = 0$ (since $q_{ij} = q_{ji}$), \eqref{eq:statprob} is obviously satisfied, so we are left with $0<i<n$, $j = i \pm 1$; $i=0$, $j=1$ and $i=n$, $j=n-1$. Noting that, for these values of $i$ and $j$,

\begin{equation}
    \dfrac{q_{ij}}{q_{ji}} = 1 \quad \mathrm{and} \quad \dfrac{\pi(j)}{\pi(i)} = \dfrac{i!(n-i)!}{j!(n-j)!} \biggl(\dfrac{1-p}{p}\biggr)^{i-j},
\end{equation}

we see that assuming

\begin{equation}
    & \alpha_{ji} = \min\biggl(\dfrac{j!(n-j)!}{i!(n-i)!} \biggl(\dfrac{1-p}{p}\biggr)^{j-i}, 1\biggr) = 1
\end{equation}

implies

\begin{equation}
    \alpha_{ij} = \dfrac{i!(n-i)!}{j!(n-j)!} \biggl(\dfrac{1-p}{p}\biggr)^{i-j} = \dfrac{\pi(j)}{\pi(i)} \dfrac{q_{ji}}{q_{ij}} \alpha_{ji},
\end{equation}

and vice versa with $\alpha_{ji} = 1$. Thus \eqref{eq:statprob} is satisfied. Since $P_{ii} > 0$ for some $i$ (namely $i = \{0, n\}$), the stationary probabilities are also the limiting probabilities. So the Markov chain converges to the desired distribution Binomial$(n,p)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\label{ssec:2b}

The following function, \texttt{binomial\_markov\_chain}, implements the Metropolis-Hastings algorithm for our specific problem:

<<echo=FALSE>>=
read_chunk("../.R/binomial_markov_chain.R")
@

<<binomial_markov_chain_function>>=
 
@

The function takes the parameters $n$ and $p$ of the desired binomial distribution and the desired length of the generated Markov chain as arguments. It returns the generated Markov chain and prints the proportion of accepted proposed states. 

\subsection{}

In the following code we use \texttt{binomial\_markov\_chain} to simulate according to a binomial distribution with n = 20 and p = 0.3 by generating a Markov chain of length 5000. Then we plot the state of the first 200 stages.

<<echo=FALSE>>=
read_chunk("../.R/plot_markov_chain.R")
require(ggplot2)
@

<<plot_markov_chain, fig.pos="htbp", fig.cap= "First 200 stages of a simulation of a Markov chain with transition probability matrix $P$, given in task 2.b">>= 
@

No clear evidence for the need of a burn-in period is presented by the plot, shown in figure \ref{fig:plot_markov_chain} - which is not surprising. Let us look into why this is so.

The sole purpose of using a burn-in period in order to propose an initial state, is that it is more likely that this state is in a high probability region of the desired distribution than if the initial state is chosen randomly (i.e. the distribution of the choice is uniform). That said, it is of course a possibility that the state returned after a burn-in period is a "worse" starting point than the initial point of the chain created during the burn-in. 

In our case, the high probability regions of the distributions are known; in fact we know the most frequent outcome of simulations of the distribution is most likely the expected value of the binomial distribution with the given parameters, which is $np = 6$. Thus the initial state proposed by a burn-in period is at best as good as state $6$, which is used as initial point in our simulation. This answers why we don't need a burn-in period.

So why is a high probability outcome a good initial point of the simulation? In the long run, i.e. as $n \rightarrow \infty$, it is insignificant what we choose as initial state, because it doesn't affect the limiting distribution of the chain. In practice, the simulation has to stop at some point. That is why choosing a low probability outcome may give a false impression of the distribution. For example, $20$ is an extremely unlikely outcome of our distribution ($\pi(20) = 0.3^{20} \approx 3.49 \cdot 10^{-11}$). With $5000$ simulations it is still very unlikely that this is one of the outcomes. Thus including this as the initial state of our Markov chain will almost certainly give a misleading impression of the form of the desired distribution, and is much more likely to do so than a simulated Markov chain with a higher probability outcome as initial state.

Now we are ready to compute the mean and variance of the simulated Markov chain: 

<<echo=TRUE>>=
mean(X)
var(X)
@

The mean and variance of Binomial$(20,0.3)$ are $np = 6$ and $np(1-p) = 4.2$ respectively. Our simulated Markov chain gives results relatively close to this. The following code plots a histogram of the relative frequencies of the different states in the simulated Markov chain.



To get more accurate results, we could  

% <<echo=FALSE, cache=FALSE>>=
% read_chunk("../.R/myrcode.R")
% @

% <<myrcode1, echo = TRUE, cache=FALSE>>=

% <<myrcode2, cache=FALSE>>=
 
% @ 

% <<echo=FALSE, cache=FALSE>>=
% source("../.R/myrcode.R")
% @

% <<cache=FALSE>>=
% summary(X)
% @